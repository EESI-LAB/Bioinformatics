---
title: "Multiple Comparisons"
author: "SW"
date: "2/13/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(ggplot2)
```

Whether you continue to work with large datasets or transition into the types of data problems one often sees stemming from benchwork, you'll come across the multiple comparisons problem. Let's focus on a situation common when working with DNA or RNA sequencing counts. Say we were given some $N \times M$ normalized gene table, where there are $N=100$ samples and $M=10000$ genes. Of these samples, 50 will belong to group 1 and the other 50 to group 2:

```{r}
N_grp1 <- 50
N_grp2 <- 50
N <- N_g1+N_g2
M <- 10000

TABLE_grp1 <- matrix(rnorm(N_grp1*M,0,1),N_grp1,M,dimnames=list(paste0('sample',1:N_grp1),paste0('gene',1:M)))
TABLE_grp2 <- matrix(rnorm(N_grp2*M,0,1),N_grp2,M,dimnames=list(paste0('sample',1:N_grp2),paste0('gene',1:M)))
TABLE <- rbind(TABLE_grp1,TABLE_grp2)
```

Now, let's aim to identify important genes that are different between the two groups. We can do this by performing a t test between the groups for each gene sequentially. I'm going to hardcode the t test there in case you are unfamiliar.

```{r}
set.seed(453)
tstats <- vector(mode='double',length=M)
for (gene in seq_len(M)){
  grp1 <- TABLE[1:50,gene]
  grp2 <- TABLE[51:100,gene]
  
  mu1 <- mean(grp1)
  mu2 <- mean(grp2)
  
  v1 <- sum((grp1 - mean(grp1))^2)
  v2 <- sum((grp2 - mean(grp2))^2)
  
  s2 <- (1/(N-2))*(v1 + v2)
  se <- sqrt(s2*(1/N_grp1 + 1/N_grp2))
  
  tstats[gene] <- (mu1-mu2)/se
}
```

And now we can plot it.

```{r}
qplot(tstats,geom='histogram',color=I('black'),bins=50)
```


Clearly the t statistics are normally distributed. The typical approach with t tests is to perform a hypothesis test at the 5% significance level, such that we reject any null hypothesis with t statistics more than 2 standard deviations from the mode in either direction of this distribution. We use 2 standard deviations because qnorm(0.975) is `r round(qnorm(0.975),3)` and qnorm(0.025) is `r round(qnorm(0.025),3)`:

```{r}
qplot(tstats,geom='histogram',color=I('black'),bins=50) +
  geom_vline(xintercept=qnorm(0.025),linetype=2,color='red') +
  geom_vline(xintercept=qnorm(0.975),linetype=2,color='red')
```

It should be apparent that there are quite a few significant genes. If you were paying attention to how our groups were generated, this should be surprising to you. We generated both groups from the *same* normal distribution. Nevertheless, we still have a ton of significant genes:

```{r}
sum(tstats<qnorm(0.025) | tstats>qnorm(0.975))
```

So over 500 genes were significant despite the fact that we know there is no underlying effect. Well, this is actually quite consistent with the defiinition of a p value. Given a typical hypothesis test, we reject the null if the probability of the observation is less than the probability of the observation occuring assuming the null hypothesis is true. In other words, we reject the claim that there is no statistical effect if our p value is less than (in absolute value) alpha, the probability of a statistical effect due to random sampling variation. Therefore, because there are 10,000 genes and hence we perform 10,000 t tests, givin an alpha of 0.05, we should expect about $0.05 \times 10000 = 500$ significant t statistics due to random sampling variation alone, which is the case here.

So that's all well and good. We now know that if we set alpha at a particular level, given our null and sample size, we should expect some false positives. But is that enough information? Let's rerun the above model, but now with an actual difference between groups:

```{r}
N_grp1 <- 50
N_grp2 <- 50
N <- N_g1+N_g2
M <- 10000

TABLE_grp1 <- matrix(rnorm(N_grp1*M,.25,2),N_grp1,M,dimnames=list(paste0('sample',1:N_grp1),paste0('gene',1:M)))
TABLE_grp2 <- matrix(rnorm(N_grp2*M,-.25,2),N_grp2,M,dimnames=list(paste0('sample',1:N_grp2),paste0('gene',1:M)))
TABLE <- rbind(TABLE_grp1,TABLE_grp2)

tstats <- vector(mode='double',length=M)
tstats2 <- vector(mode='double',length=M)
for (gene in seq_len(M)){
  grp1 <- TABLE[1:50,gene]
  grp2 <- TABLE[51:100,gene]
  
  mu1 <- mean(grp1)
  mu2 <- mean(grp2)
  
  v1 <- sum((grp1 - mean(grp1))^2)
  v2 <- sum((grp2 - mean(grp2))^2)
  
  s2 <- (1/(N-2))*(v1 + v2)
  se <- sqrt(s2*(1/N_grp1 + 1/N_grp2))
  
  tstats[gene] <- (mu1-mu2)/se
}

sum(tstats<qnorm(0.025) | tstats>qnorm(0.975))
```

Now the two groups have different normlized gene values, which leads to over 2000 significant genes. We know from before that about 500 should be due to random sampling variation, but how do we know which ones? This brings us to methods to adjust our results such that we can interpret them more easily.

## Bonferroni

We'll start with Bonferroni correction. First thing we'll do is calculate the p values for all of those t statistics:

```{r}
pvals <- 2*(1-pt(tstats,N-2))
```

Let's continue to assume alpha=0.05. Now, we should have *about* the same number of significant genes as mentioned before:

```{r}
alpha <- .05
sum(pvals < alpha)
```

Bonferroni corrects for multiple comparisons by dividing our alpha by the number of tests we performed, which gives us a new, adjusted alpha to use as a significance threshold:

```{r}
alpha_new <- alpha/M
sum(pvals < alpha_new)
```

This gives us only 2 significant genes. We should expect more given the way we set the group means. The issue with using Bonferonni here is that it's quite **conservative**, and tends to really **decrease statistical power** when there are a lot of tests, as in this case. Consequently, we are likely to see a ton of **false negatives**. In the cases where we perform only a few tests (say 10 or less), Bonferonni is quick and easy, and probably appropriate, but definitely not here.


